---
title: "Reducing Missingness Bias in ML Model Interpretability"
excerpt: "Working on reducing Out of Distribution Bias in interpretability algorithms for black box ML models, with focus on transformers across modalities"
weight: 1
---

{: .notice--info}
**ðŸ”¬ Ongoing Research** - This work is currently in progress, with new results and publications coming soon.

### Publication

* **Missingness Bias Calibration in Feature Attribution Explanations** - Accepted as a conference paper at ICLR 2026
  * We introduce MCal, a lightweight post-hoc method that corrects missingness bias by fine-tuning a simple linear head on the outputs of a frozen base model. MCal is model-agnostic, requires only output logits, and comes with theoretical guarantees of convergence to a globally optimal solution. We demonstrate its effectiveness across diverse medical benchmarks spanning vision, language, and tabular domains, where it is competitive with or outperforms prior heavyweight approaches such as full retraining and architecture modifications.
  * [Preprint (PDF)](/assets/files/MCal.pdf)

### Overview

* Working on reducing the bias in interpretability algorithms for black box machine learning models, especially transformers across text, vision and tabular modalities

* Showed and reduced bias in LLM variants (e.g., Llama, Mistral) using prompt engineering, statistical and optimization techniques

* Developing novel techniques to identify and mitigate biases in feature attribution methods

* Won runner-up award for best thesis in my department

### Status
Current ongoing research with the Brachiolab at the University of Pennsylvania

### Thesis

* [Thesis](https://drive.google.com/file/d/1IJQO_q1ob7SADcGAQG95is8SH4CyIuMB/view?usp=sharing)
